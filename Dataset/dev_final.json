[{"id":"1809.00832v1","table_header":["Batch size","Throughput (instances\/s) Inference","Throughput (instances\/s) Inference","Throughput (instances\/s) Inference","Throughput (instances\/s) Training","Throughput (instances\/s) Training","Throughput (instances\/s) Training"],"table":[["Batch size","Iter","Recur","Fold","Iter","Recur","Fold"],["1","19.2","81.4","16.5","2.5","4.8","9.0"],["10","49.3","217.9","52.2","4.0","4.2","37.5"],["25","72.1","269.9","61.6","5.5","3.6","54.7"]],"caption":"Table 2: Throughput for processing the TreeLSTM model on our recursive framework, Fold’s folding technique, and TensorFlow’s iterative approach, with the Large Movie Review dataset. The recursive approach performs the best on inference with efficient parallel execution of tree nodes, while the folding technique shows better performance on training thanks to its GPU exploitation.","description":"Table 2 compares the throughput of performing inference and training on the TreeLSTM model using our implementation, the iterative approach, and the folding technique. The amount of resources is sufficient for executing forward computations, and therefore our framework outperforms the folding technique for the inference task with up to 4.93x faster throughput.  As a result, the folding technique performs better than the recursive approach for the training task.","qa":[{"ques":"What is the throughput on inference using fold’s folding technique on batch size of 10?","ans":"52.2","tag":"Cell Selection(I)"},{"ques":"What is the highest throughput during training?","ans":"61.6","tag":"Selection by rank"},{"ques":"What is the difference between highest and lowest throughput observed?","ans":"52.6","tag":"Basic arithmetic operators"},{"ques":"Is throughput of inference for a batch size of 10 greater than throughput of training for batch size of 25?","ans":"less than","tag":"Logical operation"}]},{"id":"1809.00832v1","table_header":["Batch size","Throughput (instances\/s) Balanced","Throughput (instances\/s) Moderate","Throughput (instances\/s) Linear"],"table":[["1","46.7","27.3","7.6"],["10","125.2","78.2","22.7"],["25","129.7","83.1","45.4"]],"caption":"Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.","description":"Table 1 shows the throughput of training the TreeRNN model using these three datasets. For all batch sizes, the training throughput on the balanced dataset is the highest, while the throughput on the linear dataset is the lowest.  As a result, our implementation can train input data of balanced trees with greater throughput than input data of unbalanced trees.  Another interesting fact in Table 1 is that the training throughput on the linear dataset scales better than the throughput on the balanced dataset, as the batch size increases.  On the contrary, for the linear dataset, the recursive implementation fails to efficiently make use of CPU resources and thus the performance gain provided by increasing the batch size is relatively high.","qa":[{"ques":"What is the throughput using linear datasets on batch size of 10?","ans":"22.7","tag":"Cell Selection(I)"},{"ques":"What is the highest throughput observed?","ans":"129.7","tag":"Selection by rank"},{"ques":"For which dataset is the third least throughput observed and what is the value?","ans":"Dataset-Moderate, Value-27.3","tag":"Ordering\/sorting, Selection by rank"},{"ques":"What is the sum of least and highest throughput observed?","ans":"137.3","tag":"Basic arithmetic operators"}]},{"id":"1805.11461v1","table_header":["Representation","Hyper parameters Filter size","Hyper parameters Num. Feature maps","Hyper parameters Activation func.","Hyper parameters L2 Reg.","Hyper parameters Learning rate","Hyper parameters Dropout Prob.","F1.(avg. in 5-fold) with default values","F1.(avg. in 5-fold) with optimal values"],"table":[["CoNLL08","4-5","1000","Softplus","1.15e+01","1.13e-03","1","73.34","74.49"],["SB","4-5","806","Sigmoid","8.13e-02","1.79e-03","0.87","72.83","75.05"],["UD v1.3","5","716","Softplus","1.66e+00","9.63E-04","1","68.93","69.57"]],"caption":"Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.","description":"Table 2 presents the optimal values for each configuration using different dependency representations. We see that the optimized parameter settings vary for the different representations, showing the importance of tuning for these types of comparisons. The results furthermore show that the sdps based on the Stanford Basic (SB) representation provide the best performance, followed by the CoNLL08 representation. We observe that the results for the UD representation are quite a bit lower than the two others.","qa":[{"ques":"What is the F1 score obtained for SB representation with default values?","ans":"73.34","tag":"Cell Selection(I)"},{"ques":"What is the difference in F1 score with optimal and default values for SB representation?","ans":"2.22","tag":"Basic arithmetic operators"},{"ques":"Least F1 score is observed for which representation?","ans":"UD v1.3","tag":"Selection by rank"},{"ques":"Which representation has the highest F1 score with default values?","ans":"CoNLL08","tag":"Selection by rank"}]},{"id":"1805.11461v1","table_header":["Relation","best F1 (in 5-fold) without sdp","best F1 (in 5-fold) with sdp","Diff."],"table":[["USAGE","60.34","80.24","+ 19.90"],["MODEL-FEATURE","48.89","70.00","+ 21.11"],["PART_WHOLE","29.51","70.27","+40.76"],["TOPIC","45.80","91.26","+45.46"],["RESULT","54.35","81.58","+27.23"],["COMPARE","20.00","61.82","+ 41.82"],["macro-averaged","50.10","76.10","+26.00"]],"caption":"Table 1: Effect of using the shortest dependency path on each relation type.","description":"We find that the effect of syntactic structure varies between the different relation types. However, the sdp information has a clear positive impact on all the relation types (Table 1).","qa":[{"ques":"Which relation type gives best F1 score without sdp?","ans":"USAGE","tag":"Selection by rank"},{"ques":"Which relation type gives the least F1 score with sdp?","ans":"COMPARE","tag":"Selection by rank"},{"ques":"On which relation type does sdp show the most effect?","ans":"TOPIC","tag":"Cell Selection(II)"},{"ques":"What is the diff value for RESULT relation type?","ans":"+27.23","tag":"Cell Selection(I)"}]},{"id":"1704.06104v2","table_header":["[EMPTY]","C-F1 100%","C-F1 50%","R-F1 100%","R-F1 50%","F1 100%","F1 50%"],"table":[["Y-3","49.59","65.37","26.28","37.00","34.35","47.25"],["Y-3:YC-1","54.71","66.84","28.44","37.35","37.40","47.92"],["Y-3:YR-1","51.32","66.49","26.92","37.18","35.31","47.69"],["Y-3:YC-3","54.58","67.66","30.22","40.30","38.90","50.51"],["Y-3:YR-3","53.31","66.71","26.65","35.86","35.53","46.64"],["Y-3:YC-1:YR-2","52.95","67.84","27.90","39.71","36.54","50.09"],["Y-3:YC-3:YR-3","54.55","67.60","28.30","38.26","37.26","48.86"]],"caption":"Table 3: Performance of MTL sequence tagging approaches, essay level. Tasks separated by “:”. Layers from which tasks feed are indicated by respective numbers.","description":"Accordingly, as Table 3 shows for the essay level (paragraph level omitted for space reasons), results are generally weaker:  as in Eq. (1)—the overall result is worst. We find that when we train STagBL with only its main task—with label set  In Y contrast, when we include the 'natural subtasks' \"C\" (label  performance increases typically by a few percentage points.","qa":[{"ques":"What is the C-F1 under 50% column for y-3:yc-1?","ans":"66.84","tag":"Cell Selection(I)"},{"ques":"What is the R-F1 under 100% column for y-3:yc-3?","ans":"30.22","tag":"Cell Selection(I)"},{"ques":"What is the highest C-F1 under 50% column observed?","ans":"67.84","tag":"Selection by rank"},{"ques":"What is the least F1 under 100% column observed?","ans":"34.35","tag":"Selection by rank"}]},{"id":"1704.06104v2","table_header":["[EMPTY]","Paragraph level Acc.","Paragraph level C-F1","Paragraph level C-F1","Paragraph level R-F1","Paragraph level R-F1","Paragraph level F1","Paragraph level F1","Essay level Acc.","Essay level C-F1","Essay level C-F1","Essay level R-F1","Essay level R-F1","Essay level F1","Essay level F1"],"table":[["[EMPTY]","[EMPTY]","100%","50%","100%","50%","100%","50%","[EMPTY]","100%","50%","100%","50%","100%","50%"],["MST-Parser","31.23","0","6.90","0","1.29","0","2.17","[EMPTY]","[EMPTY]","[EMPTY]","[EMPTY]","[EMPTY]","[EMPTY]","[EMPTY]"],["Mate","22.71","2.72","12.34","2.03","4.59","2.32","6.69","[EMPTY]","[EMPTY]","[EMPTY]","[EMPTY]","[EMPTY]","[EMPTY]","[EMPTY]"],["Kiperwasser","52.80","26.65","61.57","15.57","34.25","19.65","44.01","[EMPTY]","[EMPTY]","[EMPTY]","[EMPTY]","[EMPTY]","[EMPTY]","[EMPTY]"],["LSTM-Parser","55.68","58.86","68.20","35.63","40.87","44.38","51.11","[EMPTY]","[EMPTY]","[EMPTY]","[EMPTY]","[EMPTY]","[EMPTY]","[EMPTY]"],["STagBLCC","59.34","66.69","74.08","39.83","44.02","49.87","55.22","60.46","63.23","69.49","34.82","39.68","44.90","50.51"],["LSTM-ER","61.67","70.83","77.19","45.52","50.05","55.42","60.72","54.17","66.21","73.02","29.56","32.72","40.87","45.19"],["ILP","60.32","62.61","73.35","34.74","44.29","44.68","55.23","[EMPTY]","[EMPTY]","[EMPTY]","[EMPTY]","[EMPTY]","[EMPTY]","[EMPTY]"]],"caption":"Table 2: Performance of dependency parsers, STagBLCC, LSTM-ER and ILP (from top to bottom). The ILP model operates on both levels. Best scores in each column in bold (signific. at p<0.01; Two-sided Wilcoxon signed rank test, pairing F1 scores for documents). We also report token level accuracy.","description":"We train and test all parsers on the paragraph level,  Mate is slightly better  Kiperwasser performs decently on the approximate match level, but not on exact level.  The best parser by far is the LSTM-Parser. It is over 100% better than Kiperwasser on exact spans and still several percentage points on approximate spans.  : Performance of dependency parsers, STagBLCC, LSTM-ER and ILP (from top to bottom).  On the other hand, our results in Table 2 indicate that the neural taggers BLCC and BLC (in the LSTMER model) are much better at such exact identification than either the ILP model or the neural parsers.","qa":[{"ques":"What is the C-F1 under 50% column for paragraph level on MST-Parser?","ans":"6.90","tag":"Cell Selection(I)"},{"ques":"What is the R-F1 under 100% column for essay level on LSTM-ER?","ans":"29.56","tag":"Cell Selection(I)"},{"ques":"What is the highest C-F1 under 50% column for paragraph level observed?","ans":"77.19","tag":"Selection by rank"},{"ques":"What is the highest F1 value for essay level observed?","ans":"50.51","tag":"Selection by rank"}]},{"id":"1704.06104v2","table_header":["[EMPTY]","STagBLCC","LSTM-Parser"],"table":[["Essay","60.62±3.54","9.40±13.57"],["Paragraph","64.74±1.97","56.24±2.87"]],"caption":"Table 4: C-F1 (100%) in % for the two indicated systems; essay vs. paragraph level. Note that the mean performances are lower than the majority performances over the runs given in Table 2.","description":"These results detail that the taggers have lower standard deviations than the parsers. The difference is particularly striking on the essay level where the parsers often completely fail to learn, that is, their performance scores are close to 0%.","qa":[{"ques":"Taggers or parsers, which have the lower standard deviation?","ans":"Taggers","tag":"Cell Selection(II), selection by rank"},{"ques":"What is the highest standard of deviation observed?","ans":"±13.57","tag":"Cell Selection(II), selection by rank"},{"ques":"For essay level, what is the value for LSTM-Parser?","ans":"9.40±13.57","tag":"Cell Selection(I)"},{"ques":"For paragraph level, what is the value for STagBL?","ans":"64.74±1.97","tag":"Cell Selection(I)"}]},{"id":"1911.03905v1","table_header":["Train","Test","System","BLEU","NIST","METEOR","ROUGE-L","CIDEr","Add","Miss","Wrong","SER"],"table":[["Original","Cleaned","TGen−","36.85","5.3782","35.14","55.01","1.6016","00.34","09.81","00.15","10.31"],["Original","Cleaned","TGen","39.23","6.0217","36.97","55.52","1.7623","00.40","03.59","00.07","04.05"],["Original","Cleaned","TGen+","40.25","6.1448","37.50","56.19","1.8181","00.21","01.99","00.05","02.24"],["Original","Cleaned","SC-LSTM","23.88","3.9310","32.11","39.90","0.5036","07.73","17.76","09.52","35.03"],["1-1[0.5pt\/2pt]3-12[0.5pt\/2pt] Cleaned","Cleaned","TGen−","40.19","6.0543","37.38","55.88","1.8104","00.17","01.31","00.25","01.72"],["1-1[0.5pt\/2pt]3-12[0.5pt\/2pt] Cleaned","Cleaned","TGen","40.73","6.1711","37.76","56.09","1.8518","00.07","00.72","00.08","00.87"],["1-1[0.5pt\/2pt]3-12[0.5pt\/2pt] Cleaned","Cleaned","TGen+","40.51","6.1226","37.61","55.98","1.8286","00.02","00.63","00.06","00.70"],["1-1[0.5pt\/2pt]3-12[0.5pt\/2pt] Cleaned","Cleaned","SC-LSTM","23.66","3.9511","32.93","39.29","0.3855","07.89","15.60","08.44","31.94"],["Cleaned missing","Cleaned","TGen−","40.48","6.0269","37.26","56.19","1.7999","00.43","02.84","00.26","03.52"],["Cleaned missing","Cleaned","TGen","41.57","6.2830","37.99","56.36","1.8849","00.37","01.40","00.09","01.86"],["Cleaned missing","Cleaned","TGen+","41.56","6.2700","37.94","56.38","1.8827","00.21","01.04","00.07","01.31"],["1-1[0.5pt\/2pt]3-12[0.5pt\/2pt] Cleaned added","Cleaned","TGen−","35.99","5.0734","34.74","54.79","1.5259","00.02","11.58","00.02","11.62"],["1-1[0.5pt\/2pt]3-12[0.5pt\/2pt] Cleaned added","Cleaned","TGen","40.07","6.1243","37.45","55.81","1.8026","00.05","03.23","00.01","03.29"],["1-1[0.5pt\/2pt]3-12[0.5pt\/2pt] Cleaned added","Cleaned","TGen+","40.80","6.2197","37.86","56.13","1.8422","00.01","01.87","00.01","01.88"]],"caption":"Table 3: Results evaluated on the cleaned test set (cf. Table 2 for column details; note that the numbers are not comparable to Table 2 as the test set is different).","description":"The results for testing on cleaned data (Table 3, top half) confirm the positive impact of cleaned training data and also show that the cleaned test data is more challenging (cf. Section 3), as reflected in the lower WOMs.  However, the improved results for training and testing on clean data (i.e. seeing equally challenging examples at training and test time), suggest the increase in performance can be attributed to data accuracy rather than diversity.  Looking at the detailed results for the number of added, missing, and wrong-valued slots (Add, Miss, Wrong), we observe more deletions than insertions, i.e. the models more often fail to realise part of the MR, rather than hallucinating additional information.  However, the results in bottom halves  of Tables 2 and 3 do not support our hypothesis: we observe the main effect on SER from cleaning the missed slots, reducing both insertions and deletions. Again, one possible explanation is that cleaning the missing slots provided more complex training examples.","qa":[{"ques":"What is the BLEU value for the TGen- system and trained on the original dataset?","ans":"36.85","tag":"Cell Selection(I)"},{"ques":"What is the highest SER value observed?","ans":"35.03","tag":"Selection by rank"},{"ques":"What is the METEOR value for the TGen+ system and trained on the cleaned missing dataset?","ans":"37.94","tag":"Cell Selection(I)"},{"ques":"What is the least CIDEr value observed?","ans":"0.3855","tag":"Selection by rank"}]},{"id":"1911.03905v1","table_header":["Dataset","Part","MRs","Refs","SER(%)"],"table":[["Original","Train","4,862","42,061","17.69"],["Original","Dev","547","4,672","11.42"],["Original","Test","630","4,693","11.49"],["[0.5pt\/2pt] Cleaned","Train","8,362","33,525","(0.00)"],["[0.5pt\/2pt] Cleaned","Dev","1,132","4,299","(0.00)"],["[0.5pt\/2pt] Cleaned","Test","1,358","4,693","(0.00)"]],"caption":"Table 1: Data statistics comparison for the original E2E data and our cleaned version (number of distinct MRs, total number of textual references, SER as measured by our slot matching script, see Section 3).","description":"This resulted in 20% reduction for TRAIN and ca. 8% reduction for DEV in terms of references (see Table 1). On the other hand, the number of distinct MRs rose sharply after reannotation; the MRs also have more variance in the number of attributes. This means that the cleaned dataset is more complex overall, with fewer references per MR and more diverse MRs.","qa":[{"ques":"What is the SER(%) for the original dataset DEV part?","ans":"11.42","tag":"Cell Selection(I)"},{"ques":"What is the difference of MRs in the cleaned and original dataset?","ans":"3500","tag":"Basic arithmetic operators"},{"ques":"What is the highest SER(%) obtained?","ans":"17.69","tag":"Selection by rank"},{"ques":"Which part in the original dataset has SER(%)  just lower than the TRAIN part?","ans":"TEST","tag":"Ordering\/sorting"}]},{"id":"1911.03905v1","table_header":["Train","Test","System","BLEU","NIST","METEOR","ROUGE-L","CIDEr","Add","Miss","Wrong","SER"],"table":[["Original","Original","TGen−","63.37","7.7188","41.99","68.53","1.9355","00.06","15.77","00.11","15.94"],["Original","Original","TGen","66.41","8.5565","45.07","69.17","2.2253","00.14","04.11","00.03","04.27"],["Original","Original","TGen+","67.06","8.5871","45.83","69.73","2.2681","00.04","01.75","00.01","01.80"],["Original","Original","SC-LSTM","39.11","5.6704","36.83","50.02","0.6045","02.79","18.90","09.79","31.51"],["1-1[0.5pt\/2pt]3-12[0.5pt\/2pt] Cleaned","Original","TGen−","65.87","8.6400","44.20","67.51","2.1710","00.20","00.56","00.21","00.97"],["1-1[0.5pt\/2pt]3-12[0.5pt\/2pt] Cleaned","Original","TGen","66.24","8.6889","44.66","67.85","2.2181","00.10","00.02","00.00","00.12"],["1-1[0.5pt\/2pt]3-12[0.5pt\/2pt] Cleaned","Original","TGen+","65.97","8.6630","44.45","67.59","2.1855","00.02","00.00","00.00","00.03"],["1-1[0.5pt\/2pt]3-12[0.5pt\/2pt] Cleaned","Original","SC-LSTM","38.52","5.7125","37.45","48.50","0.4343","03.85","17.39","08.12","29.37"],["Cleaned missing","Original","TGen−","66.28","8.5202","43.96","67.83","2.1375","00.14","02.26","00.22","02.61"],["Cleaned missing","Original","TGen","67.00","8.6889","44.97","68.19","2.2228","00.06","00.44","00.03","00.53"],["Cleaned missing","Original","TGen+","66.74","8.6649","44.84","67.95","2.2018","00.00","00.21","00.03","00.24"],["1-1[0.5pt\/2pt]3-12[0.5pt\/2pt] Cleaned added","Original","TGen−","64.40","7.9692","42.81","68.87","2.0563","00.01","13.08","00.00","13.09"],["1-1[0.5pt\/2pt]3-12[0.5pt\/2pt] Cleaned added","Original","TGen","66.23","8.5578","45.12","68.87","2.2548","00.04","03.04","00.00","03.09"],["1-1[0.5pt\/2pt]3-12[0.5pt\/2pt] Cleaned added","Original","TGen+","65.96","8.5238","45.49","68.79","2.2456","00.00","01.44","00.00","01.45"]],"caption":"Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section 5.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table 3 as the test set is different.","description":"The results in Table 2 (top half) for the original setup confirm that the ranking mechanism for TGen is effective for both WOMs and SER, whereas the SC-LSTM seems to have trouble scaling to the E2E dataset. We hypothesise that this is mainly due to the amount of delexicalisation required. However, the main improvement of SER comes from training on cleaned data with up to 97% error reduction with the ranker and 94% without.11 just cleaning the training data has a much more dramatic effect than just using a semantic control mechanism, such as the reranker (0.97% vs. 4.27% SER). WOMs are slightly lower for TGen trained on the cleaned data, except for NIST, which gives more importance to matching less frequent n-grams. This suggests better preservation of content at the expense of slightly lower fluency. In other words,  However, the results in bottom halves  of Tables 2 and 3 do not support our hypothesis: we observe the main effect on SER from cleaning the missed slots, reducing both insertions and deletions. Again, one possible explanation is that cleaning the missing slots provided more complex training examples.","qa":[{"ques":"What is the BLEU value for the TGen- system and trained on the original dataset?","ans":"63.37","tag":"Cell Selection(I)"},{"ques":"What is the highest SER value observed?","ans":"31.51","tag":"Selection by rank"},{"ques":"What is the METEOR value for the TGen+ system and trained on the cleaned missing dataset?","ans":"44.84","tag":"Cell Selection(I)"},{"ques":"For which system is the least CIDEr value observed?","ans":"SC-LSTM","tag":"Selection by rank"}]},{"id":"1911.03905v1","table_header":["Training data","Add","Miss","Wrong","Disfl"],"table":[["Original","0","22","0","14"],["Cleaned added","0","23","0","14"],["Cleaned missing","0","1","0","2"],["Cleaned","0","0","0","5"]],"caption":"Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).","description":"The results in Table 4 confirm the findings of the automatic  metrics: systems trained on the fully cleaned set or the set with cleaned missing slots have nearperfect performance, with the fully-cleaned one showing a few more slight disfluencies than the other. The systems trained on the original data or with cleaned added slots clearly perform worse in terms of both semantic accuracy and fluency. All fluency problems we found were very slight and no added or wrong-valued slots were found, so missed slots are the main problem.","qa":[{"ques":"What is the disfluency for original training data?","ans":"14","tag":"Cell Selection(I)"},{"ques":"Which value, high disfluency or low disfluency, indicates better performance?","ans":"Low disfluency","tag":"Cell Selection(II)"},{"ques":"What is the difference between disfluency values of cleaned added and cleaned training data?","ans":"9","tag":"Basic arithmetic operators"},{"ques":"Which 2 types of training data have the same disfluency value?","ans":"Original and Cleaned added","tag":"Logical operations"}]},{"id":"1908.05957v2","table_header":["Model","External","B"],"table":[["Seq2SeqK (Konstas et al.,  2017 )","-","22.0"],["GraphLSTM (Song et al.,  2018 )","-","23.3"],["GCNSEQ (Damonte and Cohen,  2019 )","-","24.4"],["DCGCN(single)","-","25.9"],["DCGCN(ensemble)","-","28.2"],["TSP (Song et al.,  2016 )","ALL","22.4"],["PBMT (Pourdamghani et al.,  2016 )","ALL","26.9"],["Tree2Str (Flanigan et al.,  2016 )","ALL","23.0"],["SNRG (Song et al.,  2017 )","ALL","25.6"],["Seq2SeqK (Konstas et al.,  2017 )","0.2M","27.4"],["GraphLSTM (Song et al.,  2018 )","0.2M","28.2"],["DCGCN(single)","0.1M","29.0"],["DCGCN(single)","0.2M","31.6"],["Seq2SeqK (Konstas et al.,  2017 )","2M","32.3"],["GraphLSTM (Song et al.,  2018 )","2M","33.6"],["Seq2SeqK (Konstas et al.,  2017 )","20M","33.8"],["DCGCN(single)","0.3M","33.2"],["DCGCN(ensemble)","0.3M","35.3"]],"caption":"Table 3: Main results on AMR15 with\/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M","description":"Moreover, we compare our DCGCN(single) and DCGCN(ensemble) model with the state-of-the-art semi-supervised models on the AMR15 test set (Table 3), including non-neural methods such as TSP (Song et al., 2016), PBMT (Pourdamghani et al., 2016), Tree2Str (Flanigan et al., 2016) and SNRG (Song et al., 2017). All these non-neural models train language models on the whole Gigaword corpus. Our ensemble model gives 28.2 BLEU points without external data, which is better than them. Following Konstas et al. (2017); Song et al. (2018), we also evaluate our model using external Gigaword sentences as training data. We first use the additional data to pretrain the model, then finetune it on the gold data. Using additional 0.1M data, the single DCGCN model achieves a BLEU score of 29.0, which is higher than Seq2SeqK (Konstas et al., 2017) and GraphLSTM (Song et al., 2018) trained with 0.2M additional data. When using the same amount of 0.2M data, the performance of DCGCN is 4.2 and 3.4 BLEU points higher than Seq2SeqK and GraphLSTM. DCGCN model is able to achieve a competitive BLEU points (33.2) by using 0.3M external data, while GraphLSTM achieves a score of 33.6 by using 2M data and Seq2SeqK achieves a score of 33.8 by using 20M data. These results show that our model is more effective in terms of using automatically generated AMR graphs. Using 0.3M additional data, our ensemble model achieves the new state-of-the-art result of 35.3 BLEU points.","qa":[{"ques":"What is the BLEU score of the DCGCN ensemble model without external data?","ans":"28.2","tag":"Cell Selection(I)"},{"ques":"What is the BLEU score of the DCGCN single model trained with 0.1M extra parameters?","ans":"29.0","tag":"Cell Selection(I)"},{"ques":"Which model gives the highest BLEU score and what is the value?","ans":"DCGCN(ensemble), value-35.3","tag":"Selection by rank"},{"ques":"How many Gigaword sentences are parsed as training data in the case of highest B score?","ans":"0.3M","tag":"Cell Selection(II)"}]},{"id":"1908.05957v2","table_header":["Model","T","#P","B","C"],"table":[["Seq2SeqB (Beck et al.,  2018 )","S","28,4M","21.7","49.1"],["GGNN2Seq (Beck et al.,  2018 )","S","28.3M","23.3","50.4"],["Seq2SeqB (Beck et al.,  2018 )","E","142M","26.6","52.5"],["GGNN2Seq (Beck et al.,  2018 )","E","141M","27.5","53.5"],["DCGCN (ours)","S","19.1M","27.9","57.3"],["DCGCN (ours)","E","92.5M","30.4","59.6"]],"caption":"Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; “S” and “E” denote single and ensemble models, respectively.","description":"Table 2 shows the results on AMR17. Our single model achieves 27.6 BLEU points, which is the new state-of-the-art result for single models. In particular, our single DCGCN model consistently outperforms Seq2Seq models by a significant margin when trained without external resources. For example, the single DCGCN model gains 5.9 more BLEU points than the single models of Seq2SeqB on AMR17. These results demonstrate the importance of explicitly capturing the graph structure in the encoder. In addition, our single DCGCN model obtains better results than previous ensemble models. For example, on AMR17, the single DCGCN model is 1 BLEU point higher than the ensemble model of Seq2SeqB. Our model requires substantially fewer parameters, e.g., the parameter size is only 3\/5 and 1\/9 of those in GGNN2Seq and Seq2SeqB, respectively. The ensemble approach based on combining five DCGCN models initialized with different random seeds achieves a BLEU score of 30.4 and a CHRF++ score of 59.6. Under the same setting, our model also consistently outperforms graph encoders based on recurrent neural networks or gating mechanisms. For GGNN2Seq, our single model is 3.3 and 0.1 BLEU points higher than their single and ensemble models, respectively. We also have similar observations in term of CHRF++ scores for sentence-level evaluations. DCGCN also outperforms GraphLSTM by 2.0 BLEU points in the fully supervised setting as shown in Table 3. Note that GraphLSTM uses char-level neural represen  tations and pretrained word embeddings, while our model solely relies on word-level representations with random initializations. This empirically shows that compared to recurrent graph encoders, DCGCNs can learn better representations for graphs. For GCNSEQ, our single models are 3.1 and 1.3 BLEU points higher than their models trained on AMR17 and AMR15 dataset, respectively. These results demonstrate that DCGCNs are able to capture contextual information without relying on additional RNNs.","qa":[{"ques":"What is the C score of the single Seq2SeqB model?","ans":"49.1","tag":"Cell Selection(I)"},{"ques":"Which ensemble model gives the least C score?","ans":"Seq2SeqB model","tag":"Cell Selection(II)"},{"ques":"What is the difference between the C score of our ensemble model and GGNN2Seq ensemble model?","ans":"6.1","tag":"Basic arithmetic operators"},{"ques":"What is the B score of the single DCGCN model?","ans":"27.9","tag":"Cell Selection(I)"}]},{"id":"1908.05957v2","table_header":["Model","Type","English-German #P","English-German B","English-German C","English-Czech #P","English-Czech B","English-Czech C"],"table":[["BoW+GCN (Bastings et al.,  2017 )","Single","-","12.2","-","-","7.5","-"],["CNN+GCN (Bastings et al.,  2017 )","Single","-","13.7","-","-","8.7","-"],["BiRNN+GCN (Bastings et al.,  2017 )","Single","-","16.1","-","-","9.6","-"],["PB-SMT (Beck et al.,  2018 )","Single","-","12.8","43.2","-","8.6","36.4"],["Seq2SeqB (Beck et al.,  2018 )","Single","41.4M","15.5","40.8","39.1M","8.9","33.8"],["GGNN2Seq (Beck et al.,  2018 )","Single","41.2M","16.7","42.4","38.8M","9.8","33.3"],["DCGCN (ours)","Single"," 29.7M","19.0","44.1"," 28.3M","12.1","37.1"],["Seq2SeqB (Beck et al.,  2018 )","Ensemble","207M","19.0","44.1","195M","11.3","36.4"],["GGNN2Seq (Beck et al.,  2018 )","Ensemble","206M","19.6","45.1","194M","11.7","35.9"],["DCGCN (ours)","Ensemble"," 149M","20.5","45.8"," 142M","13.1","37.8"]],"caption":"Table 4: Main results on English-German and English-Czech datasets.","description":"Table 4 shows the results for the EnglishGerman (En-De) and English-Czech (En-Cs) translation tasks. BoW+GCN, CNN+GCN and BiRNN+GCN refer to employing the following encoders with a GCN layer on top respectively: 1) a bag-of-words encoder, 2) a one-layer CNN, 3) a bidirectional RNN. PB-SMT is the phrase-based statistical machine translation model using Moses (Koehn et al., 2007). Our single model DCGCN(single) achieves 19.0 and 12.1 BLEU points on the En-De and EnCs tasks, respectively, significantly outperforming all the single models. For example, compared to the best GCN-based model (BiRNN+GCN), our single DCGCN model surpasses it by 2.7 and 2.5 BLEU points on the En-De and En-Cs tasks, respectively. Our models DCGCN(single) and DCGCN(ensemble)consist of full GCN layers, removing the burden of employing a recurrent encoder to extract non-local contextual information in the bottom layers. Compared to non-GCN models, our single DCGCN model is 2.2 and 1.9 BLEU points higher than the current state-of-theart single model (GGNN2Seq) on the En-De and En-Cs translation tasks, respectively. In addition, our single model is comparable to the ensemble results of Seq2SeqB and GGNN2Seq, while the number of parameters of our models is only about 1\/6 of theirs. Additionally, the ensemble DCGCN models achieve 20.5 and 13.1 BLEU points on the En-De and En-Cs tasks, respectively. Our ensemble results are significantly higher than those of the state-of-the-art syntax-based ensemble models reported by GGNN2Seq (En-De: 20.5 v.s. 19.6; En-Cs: 13.1 v.s. 11.7 in terms of BLEU).","qa":[{"ques":"What is the B score of the single BoW+GCN model for English-German translation tasks?","ans":"12.2","tag":"Cell Selection(I)"},{"ques":"Which ensemble model gives the least C score for English-German translation tasks?","ans":"Seq2SeqB","tag":"Cell Selection(I)"},{"ques":"Which ensemble model gives the least C score for English-Czech translation tasks?","ans":"GGNN2Seq","tag":"Cell Selection(I)"},{"ques":"What is the difference of BLEU points between the best single GCN based model and our single model for EnCs task?","ans":"2.5","tag":"Cell Selection(II)"}]},{"id":"1908.05957v2","table_header":["Block","n","m","B","C"],"table":[["1","1","1","17.6","48.3"],["1","1","2","19.2","50.3"],["1","2","1","18.4","49.1"],["1","1","3","19.6","49.4"],["1","3","1","20.0","50.5"],["1","3","3","21.4","51.0"],["1","3","6","21.8","51.7"],["1","6","3","21.7","51.5"],["1","6","6","22.0","52.1"],["2","3","6","23.5","53.3"],["2","6","3","23.3","53.4"],["2","6","6","22.0","52.1"]],"caption":"Table 5: The effect of the number of layers inside DCGCN sub-blocks on the AMR15 development set.","description":"Layers in the sub-block. Table 5 shows the effect of the number of layers of each sub-block on the AMR15 development set. DenseNets (Huang et al., 2017) use two kinds of convolution filters: 1 × 1 and 3 × 3. Similar to DenseNets, we choose the values of n and m for layers from [1, 2, 3, 6]. We choose this value range by considering the scale of non-local nodes, the abstract information at different level and the calculation efficiency. For brevity, we only show representative configurations. We first investigate DCGCN with one block. In general, the performance increases when we gradually enlarge n and m. For example, when n=1 and m=1, the BLEU score is 17.6; when n=6 and m=6, the BLEU score becomes 22.0. We observe that the three settings (n=6, m=3), (n=3, m=6) and (n=6, m=6) give similar results for both 1 DCGCN block and 2 DCGCN blocks. Since the first two settings contain less parameters than the third setting, it is reasonable to choose either (n=6, m=3) or (n=3, m=6). For later experiments, we use (n=6, m=3).","qa":[{"ques":"What is the BLEU value for one block DCGCN n=1,m=2","ans":"19.2","tag":"Cell Selection(I)"},{"ques":"For what values of n and m is the highest BLEU score observed for one block DCGCN?","ans":"n=6,m=6","tag":"Selection by rank"},{"ques":"For what values of n and m is the highest BLEU score observed for two block DCGCN?","ans":"n=3,m=6","tag":"Selection by rank"},{"ques":"For what values of n and m is the highest C score observed for two block DCGCN and what is the value?","ans":"n=6,m=3, value-53.4","tag":"Selection by rank"}]},{"id":"1908.05957v2","table_header":["GCN +RC (2)","B 16.8","C 48.1","GCN +RC+LA (2)","B 18.3","C 47.9"],"table":[["+RC (4)","18.4","49.6","+RC+LA (4)","18.0","51.1"],["+RC (6)","19.9","49.7","+RC+LA (6)","21.3","50.8"],["+RC (9)","21.1","50.5","+RC+LA (9)","22.0","52.6"],["+RC (10)","20.7","50.7","+RC+LA (10)","21.2","52.9"],["DCGCN1 (9)","22.9","53.0","DCGCN3 (27)","24.8","54.7"],["DCGCN2 (18)","24.2","54.4","DCGCN4 (36)","25.5","55.4"]],"caption":"Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i×(n+m) layers. The number of layers for each model is shown in parenthesis.","description":"The first block in Table 6 shows the performance of our two baseline models: multi-layer GCNs with residual connections (GCN+RC) and multi-layer GCNs with both residual connections and layer aggregations (GCN+RC+LA). In general, increasing the number of GCN layers from 2 to 9 boosts the model performance. However, when the layer number exceeds 10, the performance of both baseline models start to drop. For example, GCN+RC+LA (10) achieves a BLEU score of 21.2, which is worse than GCN+RC+LA (9). In preliminary experiments, we cannot manage to train very deep GCN+RC and GCN+RC+LA models. In contrast, our DCGCN models can be trained using a large number of layers. For example, DCGCN4 contains 36 layers. When we increase the DCGCN blocks from 1 to 4, the model performance continues increasing on AMR15 development set. We therefore choose DCGCN4 for the AMR experiments. Using a similar method, DCGCN2 is selected for the NMT tasks. When the layer numbers are 9, DCGCN1 is better than GCN+RC in term of B\/C scores (21.7\/51.5 v.s. 21.1\/50.5). GCN+RC+LA (9) is sightly better than DCGCN1. However, when we set the number to 18, GCN+RC+LA achieves a BLEU score of 19.4, which is significantly worse than the BLEU score obtained by DCGCN2 (23.3). We also try GCN+RC+LA (27), but it does not converge. In conclusion, these results above can show the robustness and effectiveness of our DCGCN models.","qa":[{"ques":"How many layered GCN+RC+LA gives the highest BLEU score?","ans":"9","tag":"Cell Selection(II)"},{"ques":"How many layered GCN+RC+LA gives the highest C score?","ans":"10","tag":"Cell Selection(II)"},{"ques":"What is the BLEU score of the GCN+RC(6) model?","ans":"19.9","tag":"Cell Selection(I)"},{"ques":"Which model has the highest C value?","ans":"DCGCN4 (36)","tag":"Selection by rank"}]},{"id":"1908.05957v2","table_header":["Model","D","#P","B","C"],"table":[["DCGCN(1)","300","10.9M","20.9","52.0"],["DCGCN(2)","180","10.9M","22.2","52.3"],["DCGCN(2)","240","11.3M","22.8","52.8"],["DCGCN(4)","180","11.4M","23.4","53.4"],["DCGCN(1)","420","12.6M","22.2","52.4"],["DCGCN(2)","300","12.5M","23.8","53.8"],["DCGCN(3)","240","12.3M","23.9","54.1"],["DCGCN(2)","360","14.0M","24.2","54.4"],["DCGCN(3)","300","14.0M","24.4","54.2"],["DCGCN(2)","420","15.6M","24.1","53.7"],["DCGCN(4)","300","15.6M","24.6","54.8"],["DCGCN(3)","420","18.6M","24.5","54.6"],["DCGCN(4)","360","18.4M","25.5","55.4"]],"caption":"Table 7: Comparisons of different DCGCN models under almost the same parameter budget.","description":"We compare DCGCN models with different layers under the same parameter budget. Table 7 shows the results. For example, when both DCGCN1 and DCGCN2 are limited to 10.9M parameters, DCGCN2 obtains 22.2 BLEU points, which is higher than DCGCN1 (20.9). Similarly, when DCGCN3 and DCGCN4 contain 18.6M and 18.4M parameters. DCGCN4 outperforms DCGCN3 by 1 BLEU point with a slightly smaller model. In general, we found when the parameter budget is the same, deeper DCGCN models can obtain better results than the shallower ones.","qa":[{"ques":"Which model, DCGCN(3) with 18.6M parameters or DCGCN(4) with 18.4M parameters, performs better?","ans":"DCGCN(4) with 18.4M parameters","tag":"Logical Operations"},{"ques":"What is the difference of BLEU scores of above models?","ans":"1","tag":"Basic arithmetic operators"},{"ques":"What is the highest C value observed?","ans":"55.4","tag":"Selection by rank"},{"ques":"For the DCGCN(2) model with 12.5M parameters what are the B and C values?","ans":"23.8 and 53.8","tag":"Cell Selection(I)"}]},{"id":"1908.05957v2","table_header":["Model","B","C"],"table":[["DCGCN4","25.5","55.4"],["-{4} dense block","24.8","54.9"],["-{3, 4} dense blocks","23.8","54.1"],["-{2, 3, 4} dense blocks","23.2","53.1"]],"caption":"Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.","description":"Table 8 shows the ablation study of the level of density of our model DCGCN4. We use DCGCNs with 4 dense blocks as the full model. Then we remove dense connections gradually from the last block to the first block. In general, the performance of the model drops substantially as we remove more dense connections until it cannot converge without dense connections. The full model gives 25.5 BLEU points on the AMR15 dev set. After removing the dense connections in the last block, the BLEU score becomes 24.8. Without using the dense connections in the last two blocks, the score drops to 23.8. Furthermore, excluding the dense connections in the last three blocks only gives 23.2 BLEU points. Although these four models have the same number of layers, dense connections allow the model to achieve much better performance. If all the dense connections are not considered, the model does not coverage at all. These results indicate dense connections do play a significant role in our model.","qa":[{"ques":"What is the BLEU value for the DCGCN4 model?","ans":"25.5","tag":"Cell Selection(I)"},{"ques":"Removing dense connections in 3rd and 4th block results in what C value?","ans":"54.1","tag":"Cell Selection(II)"},{"ques":"For which model is the lowest C value observed?","ans":"{2, 3, 4} dense blocks","tag":"Selection by rank"},{"ques":"What is the difference in C score of the DCGCN4 model and the -{4} dense block model?","ans":"0.5","tag":"Basic arithmetic operators"}]},{"id":"1908.05957v2","table_header":["Model","B","C"],"table":[["DCGCN4","25.5","55.4"],["Encoder Modules","[EMPTY]","[EMPTY]"],["-Linear Combination","23.7","53.2"],["-Global Node","24.2","54.6"],["-Direction Aggregation","24.6","54.6"],["-Graph Attention","24.9","54.7"],["-Global Node&Linear Combination","22.9","52.4"],["Decoder Modules","[EMPTY]","[EMPTY]"],["-Coverage Mechanism","23.8","53.0"]],"caption":"Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder","description":"Table 9 shows the results. For the encoder, we find that the linear combination and the global node have more contributions in terms of B\/C scores. The results drop by 2\/2.2 and 1.3\/1.2 points respectively after removing them. Without these two components, our model gives a BLEU score of 22.6, which is still better than the best GCN+RC model (21.1) and the best GCN+RC+LA model (22.1). Adding either the global node or the linear combination improves the baseline models with only dense connections. This suggests that enriching input graphs with the global node and including the linear combination can facilitate GCNs to learn better information aggregations, producing more expressive graph representations. Results also show the linear combination is more effective than the global node. Considering them together further enhances the model performance. After removing the graph attention module, our model gives 24.9 BLEU points. Similarly, excluding the direction aggregation module leads to a performance drop to 24.6 BLEU points. The coverage mechanism is also effective in our models. Without the coverage mechanism, the result drops by 1.7\/2.4 points for B\/C scores.","qa":[{"ques":"What is the BLEU score for encoder modules linear combination?","ans":"23.7","tag":"Cell Selection(I)"},{"ques":"What is the C value for Decoder modules coverage mechanism?","ans":"53.0","tag":"Cell Selection(I)"},{"ques":"What is the highest C value observed?","ans":"55.4","tag":"Selection by rank"},{"ques":"Which 2 encoder module models have the same C value?","ans":"Global node and Linear combination","tag":"Logical operations"}]},{"id":"1902.06423v1","table_header":["Initialization","Depth","BShift","SubjNum","Tense","CoordInv","Length","ObjNum","TopConst","SOMO","WC"],"table":[["N(0,0.1)","29.7","71.5","82.0","78.5","60.1","80.5","76.3","74.7","51.3","52.5"],["Glorot","31.3","72.3","81.8","78.7","59.4","81.3","76.6","74.6","50.4","57.0"],["Our paper","35.1","70.8","82.0","80.2","61.8","82.8","79.7","74.2","50.7","72.9"]],"caption":"Table 7: Scores for initialization strategies on probing tasks.","description":"To verify the effectiveness of our initialization strategy empirically, we evaluate it with the same experimental setup as described in Section 4. The only difference is the initialization strategy, where we include Glorot initialization (Glorot & Bengio, 2010) and the standard initialization from  (0, 0.1). Table 7 shows the results on the probing tasks. While Glorot achieves slightly better results on BShift and TopConst, CMOW's ability to memorize word content is improved by a wide  margin by our initialization strategy.","qa":[{"ques":"What is the WC value for Glorot initialization?","ans":"57.0","tag":"Cell Selection(I)"},{"ques":"On how many tasks does Glorot initialization have highest performance?","ans":"2","tag":"Selection by rank"},{"ques":"On which task N (0, 0.1) and our paper initialization have the same performance?","ans":"SubjNum","tag":"Logical operations"},{"ques":"For which initialization does the SOMO task give the highest value?","ans":"N(0,0.1)","tag":"Selection by rank"}]},{"id":"1902.06423v1","table_header":["Dim","Method","Depth","BShift","SubjNum","Tense","CoordInv","Length","ObjNum","TopConst","SOMO","WC"],"table":[["400","CBOW\/400","32.5","50.2","78.9","78.7","53.6","73.6","79.0","69.6","48.9","86.7"],["400","CMOW\/400","34.4","68.8","80.1","79.9","59.8","81.9","79.2","70.7","50.3","70.7"],["400","H-CBOW","31.2","50.2","77.2","78.8","52.6","77.5","76.1","66.1","49.2","87.2"],["400","H-CMOW","32.3","70.8","81.3","76.0","59.6","82.3","77.4","70.0","50.2","38.2"],["784","CBOW\/784","33.0","49.6","79.3","78.4","53.6","74.5","78.6","72.0","49.6","89.5"],["784","CMOW\/784","35.1","70.8","82.0","80.2","61.8","82.8","79.7","74.2","50.7","72.9"],["800","Hybrid","35.0","70.8","81.7","81.0","59.4","84.4","79.0","74.3","49.3","87.6"],["-","cmp. CBOW","+6.1%","+42.7%","+3%","+3.3%","+10.8%","+13.3%","+0.5%","+3.2%","-0.6%","-2.1%"],["-","cmp. CMOW","-0.3%","+-0%","-0.4%","+1%","-3.9%","+1.9%","-0.9%","+0.1%","-2.8%","+20.9%"]],"caption":"Table 1: Scores on the probing tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.","description":"We have trained five different models: CBOW and CMOW with d = 20 and d = 28, which lead to 400-dimensional and 784-dimensional word embeddings, respectively. We also trained the Hybrid CBOW-CMOW model with d = 20 for each component, so that the total model has 800 parameters per word in the lookup tables. We report the results of two more models: H-CBOW is the 400-dimensional CBOW component trained in Hybrid and H-CMOW is the respective CMOW component. Below, we compare the 800-dimensional Hybrid method to the 784-dimensional CBOW and CMOW models.  Considering the linguistic probing tasks (see Table 1), CBOW and CMOW show complementary results. While CBOW yields the highest performance at word content memorization, CMOW outperforms CBOW at all other tasks. Most improvements vary between 1-3 percentage points. The difference is approximately 8 points for CoordInv and Length, and even 21 points for BShift. The hybrid model yields scores close to or even above the better model of the two on all tasks. In terms of relative numbers, the hybrid model improves upon CBOW in all probing tasks but WC and SOMO. The relative improvement averaged over all tasks is 8%. Compared to CMOW, the hybrid model shows rather small differences. The largest loss is by 4% on the CoordInv task. However, due to the large gain in WC (20.9%), the overall average gain is still 1.6%. We now compare the jointly trained H-CMOW and H-CBOW with their separately trained 400dimensional counterparts. We observe that CMOW loses most of its ability to memorize word content, while CBOW shows a slight gain. On the other side, H-CMOW shows, among others, improvements at BShift.  Regarding the probing tasks, we observe that CMOW embeddings better encode the linguistic prop  erties of sentences than CBOW.  Due to joint training, our hybrid model learns to pick up the best features from CBOW and CMOW simultaneously. It enables both models to focus on their respective strengths. This can best be seen by observing that H-CMOW almost completely loses its ability to memorize word content. In return, H-CMOW has more capacity to learn other properties, as seen in the increase in performance at BShift and others. A complementary behavior can be observed for H-CBOW, whose scores on Word Content are increased. Consequently, with an 8% improvement on average, the hybrid model  Word Content are increased. Consequently, with an 8% i is substantially more linguistically informed than CBOW.","qa":[{"ques":"What is the WC value for H-CMOW method for 400 dimensional word embedding?","ans":"38.2","tag":"Cell Selection(I)"},{"ques":"What is the only task at which CBOW gives better performance than CMOW?","ans":"Word content memorization","tag":"Cell Selection(II)"},{"ques":"What is the highest WC value observed?","ans":"89.5","tag":"Selection by rank"},{"ques":"What are the highest TopConst and SOMO values observed?","ans":"74.3 and 50.7","tag":"Selection by rank"}]},{"id":"1902.06423v1","table_header":["Method","SUBJ","CR","MR","MPQA","MRPC","TREC","SICK-E","SST2","SST5","STS-B","SICK-R"],"table":[["CBOW\/784","90.0","79.2","74.0","87.1","71.6","85.6","78.9","78.5","42.1","61.0","78.1"],["CMOW\/784","87.5","73.4","70.6","87.3","69.6","88.0","77.2","74.7","37.9","56.5","76.2"],["Hybrid","90.2","78.7","73.7","87.3","72.7","87.6","79.4","79.6","43.3","63.4","77.8"],["cmp. CBOW","+0.2%","-0.6%","-0.4%","+0.2%","+1.5%","+2.3%","+0.6%","+1.4%","+2.9%","+3.9%","-0.4%"],["cmp. CMOW","+3.1%","+7.2%","+4.4%","+0%","+4.5%","-0.5%","+2.9%","+6.7%","+14.3","+12.2%","+2.1%"]],"caption":"Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.","description":"Table 2 shows the scores from the supervised downstream tasks. Comparing the 784-dimensional models, again, CBOW and CMOW seem to complement each other. This time, however, CBOW has the upperhand, matching or outperforming CMOW on all supervised downstream tasks except  TREC by up to 4 points. On the TREC task, on the other hand, CMOW outperforms CBOW by 2.5 points. Our jointly trained model is not more than 0.8 points below the better one of CBOW and CMOW on any of the considered supervised downstream tasks. On 7 out of 11 supervised tasks, the joint model even improves upon the better model, and on SST2, SST5, and MRPC the difference is more than 1 point. The average relative improvement over all tasks is 1.2%.  Our CMOW model produces sentence embeddings that are approximately at the level of fast  Sent (Hill et al., 2016). Thus, CMOW is a reasonable choice as a sentence encoder.  observe that CMOW embeddings better encode the linguistic propCMOW gets reasonably close to CBOW on some downstream tasks.  However, CMOW does not in general supersede CBOW embeddings.","qa":[{"ques":"What is the SICK-R value for CMOW method for 784 dimensional word embedding?","ans":"76.2","tag":"Cell Selection(I)"},{"ques":"On which downstream tasks does the CBOW method have the highest score?","ans":"CR,MR,SICK-R","tag":"Selection by rank"},{"ques":"Which model gives the best performance on SUBJ task?","ans":"Hybrid","tag":"Selection by rank"},{"ques":"On which downstream task do 2 methods give the same performance?","ans":"MPQA","tag":"Logical operations"}]},{"id":"1902.06423v1","table_header":["Method","STS12","STS13","STS14","STS15","STS16"],"table":[["CBOW","43.5","50.0","57.7","63.2","61.0"],["CMOW","39.2","31.9","38.7","49.7","52.2"],["Hybrid","49.6","46.0","55.1","62.4","62.1"],["cmp. CBOW","+14.6%","-8%","-4.5%","-1.5%","+1.8%"],["cmp. CMOW","+26.5%","+44.2%","+42.4","+25.6%","+19.0%"]],"caption":"Table 3: Scores on unsupervised downstream tasks attained by our models. Rows starting with “Cmp.” show the relative change with respect to Hybrid.","description":"Regarding the unsupervised downstream tasks (Table 3), CBOW is clearly superior to CMOW on all datasets by wide margins. For example, on STS13, CBOW's score is 50% higher. The hybrid model is able to repair this deficit, reducing the difference to 8%. It even outperforms CBOW on two of the tasks, and yields a slight improvement of 0.5% on average over all unsupervised downstream tasks. However, the variance in relative performance is notably larger than on the supervised downstream tasks.","qa":[{"ques":"What is the STS16 score for the CMOW model?","ans":"52.2","tag":"Cell Selection(I)"},{"ques":"What is the relative change with respect to hybrid for CBOW model on STS13 downstream task?","ans":"-8%","tag":"Cell Selection(II)"},{"ques":"What is the relative change with respect to hybrid for CMOW model on STS14 downstream task?","ans":"+42.4%","tag":"Cell Selection(II)"},{"ques":"On which unsupervised downstream tasks does CBOW method give the best performance?","ans":"STS13,STS14 and STS15","tag":"Selection by rank"}]},{"id":"1902.06423v1","table_header":["Initialization","SUBJ","CR","MR","MPQA","MRPC","TREC","SICK-E","SST2","SST5","STS-B","SICK-R"],"table":[["N(0,0.1)","85.6","71.5","68.4","86.2","71.6","86.4","73.7","72.3","38.2","53.7","72.7"],["Glorot","86.2","74.4","69.5","86.5","71.4","88.4","75.4","73.2","38.2","54.1","73.6"],["Our paper","87.5","73.4","70.6","87.3","69.6","88.0","77.2","74.7","37.9","56.5","76.2"]],"caption":"Table 8: Scores for initialization strategies on supervised downstream tasks.","description":"the success of our training schema for the CMOW model are two changes to the original word2vec training. First, our initialization strategy improved the downstream performance by 2.8% compared to Glorot initialization.","qa":[{"ques":"What is the SICK-R value for Glorot initialization?","ans":"73.6","tag":"Cell Selection(I)"},{"ques":"On how many tasks does our paper initialization have highest performance?","ans":"7","tag":"Selection by rank and Basic arithmetic operators"},{"ques":"On which task N (0, 0.1) and Glorot initialization have the same performance?","ans":"SST5","tag":"Logical operations"},{"ques":"What is the highest TREC score observed and for which initialization?","ans":"88.4 for Glorot initialization","tag":"Selection by rank"}]},{"id":"1902.06423v1","table_header":["Method","STS12","STS13","STS14","STS15","STS16"],"table":[["CMOW-C","27.6","14.6","22.1","33.2","41.6"],["CMOW-R","39.2","31.9","38.7","49.7","52.2"],["CBOW-C","43.5","49.2","57.9","63.7","61.6"],["CBOW-R","43.5","50.0","57.7","63.2","61.0"]],"caption":"Table 6: Scores for different training objectives on the unsupervised downstream tasks.","description":"alization strategy improved the downstream performance by 2.8% compared Secondly, by choosing the target word of the objective at random, the perfor  mance of CMOW on downstream tasks improved by 20.8% on average.  and on all unsupervised downstream tasks","qa":[{"ques":"What is the STS16 value for the CMOW-C method?","ans":"41.6","tag":"Cell Selection(I)"},{"ques":"On which unsupervised downstream tasks does CBOW-C and CBOW-R have the same value?","ans":"STS12","tag":"Cell Selection(II)"},{"ques":"What is the value observed in the above question?","ans":"43.5","tag":"Cell Selection(I)"},{"ques":"For which method is the highest STS15 value observed?","ans":"CBOW-C","tag":"Selection by rank"}]},{"id":"1902.06423v1","table_header":["Method","Depth","BShift","SubjNum","Tense","CoordInv","Length","ObjNum","TopConst","SOMO","WC"],"table":[["CMOW-C","36.2","66.0","81.1","78.7","61.7","83.9","79.1","73.6","50.4","66.8"],["CMOW-R","35.1","70.8","82.0","80.2","61.8","82.8","79.7","74.2","50.7","72.9"],["CBOW-C","34.3","50.5","79.8","79.9","53.0","75.9","79.8","72.9","48.6","89.0"],["CBOW-R","33.0","49.6","79.3","78.4","53.6","74.5","78.6","72.0","49.6","89.5"]],"caption":"Table 4: Scores for different training objectives on the linguistic probing tasks.","description":"To test the effectiveness of this modified objective, we evaluate it with the same experimental setup as described in Section 4. Table 4 lists the results on the linguistic probing tasks. CMOW-C and CBOW-C refer to the models where the center word is used as the target. CMOW-R and CBOW-R refer to the models where the target word is sampled randomly. While CMOW-R and CMOW-C perform comparably on most probing tasks, CMOW-C yields 5 points lower scores on WordContent  and BigramShift.","qa":[{"ques":"What is the WC value for CMOW-R method?","ans":"72.9","tag":"Cell Selection(I)"},{"ques":"What is the highest SOMO value observed?","ans":"50.7","tag":"Selection by rank"},{"ques":"On how many linguistic probing tasks does CMOW-C perform better than CMOW-R?","ans":"2","tag":"Cell Selection(II) and Basic arithmetic operators"},{"ques":"On how many linguistic probing tasks does CBOW-R perform better than CBOW-C?","ans":"3","tag":"Cell Selection(II) and Basic arithmetic operators"}]},{"id":"1902.06423v1","table_header":["Method","SUBJ","CR","MR","MPQA","MRPC","TREC","SICK-E","SST2","SST5","STS-B","SICK-R"],"table":[["CMOW-C","85.9","72.1","69.4","87.0","71.9","85.4","74.2","73.8","37.6","54.6","71.3"],["CMOW-R","87.5","73.4","70.6","87.3","69.6","88.0","77.2","74.7","37.9","56.5","76.2"],["CBOW-C","90.0","79.3","74.6","87.5","72.9","85.0","80.0","78.4","41.0","60.5","79.2"],["CBOW-R","90.0","79.2","74.0","87.1","71.6","85.6","78.9","78.5","42.1","61.0","78.1"]],"caption":"Table 5: Scores for different training objectives on the supervised downstream tasks.","description":"Consequently, CMOW-R also outperforms CMOW-C on 10 out of 11 supervised  downstream tasks  On average over all downstream tasks, the relative improvement is 20.8%.  scores on downstream tasks increase on some tasks and decrease on others. The differences ar miniscule. On average over all 16 downstream tasks, CBOW-R scores 0.1% lower than CBOW-C.  For CBOW, the","qa":[{"ques":"What is the SICK-E value for the CMOW-R method?","ans":"77.2","tag":"Cell Selection(I)"},{"ques":"What is the highest MPQA value observed?","ans":"87.5","tag":"Selection by rank"},{"ques":"On how many supervised downstream tasks does CMOW-C perform better than CMOW-R?","ans":"1","tag":"Cell Selection(II) and Basic arithmetic operators"},{"ques":"On how many supervised downstream tasks does CBOW-R perform better than CBOW-C?","ans":"5","tag":"Cell Selection(II) and Basic arithmetic operators"}]},{"id":"1905.07189v2","table_header":["System","All LOC","All ORG","All PER","All MISC","In  E+ LOC","In  E+ ORG","In  E+ PER","In  E+ MISC"],"table":[["Name matching","96.26","89.48","57.38","96.60","92.32","76.87","47.40","76.29"],["MIL","57.09","76.30","41.35","93.35","11.90","47.90","27.60","53.61"],["MIL-ND","57.15","77.15","35.95","92.47","12.02","49.77","20.94","47.42"],["τMIL-ND","55.15","76.56","34.03","92.15","11.14","51.18","20.59","40.00"],["Supervised learning","55.58","61.32","24.98","89.96","8.80","14.95","7.40","29.90"]],"caption":"Table 3: % errors on the development set for different named entity types under two settings. (Smaller is better.)","description":"In Table 3 we classified errors according to named entity types  PER is the easiest type for all systems. Even name matching, without any learning, can correctly predict in half of the cases.  For LOC, it turns out that candidate selection is a bottleneck: when candidate selection was flawless, the models made only about 12% errors, down from about 57%. For MISC a similar conclusion can be drawn.","qa":[{"ques":"What is the value of MISC under the In E+ setting for MIL system?","ans":"53.61","tag":"Cell Selection(I)"},{"ques":"Under All setting highest error for LOC was observed for which system?","ans":"Name matching system","tag":"Selection by rank"},{"ques":"What is the value of ORG under the All setting for the MIL-ND system?","ans":"77.15","tag":"Cell Selection(I)"},{"ques":"Under In E+ setting lowest error for PER was observed for which system?","ans":"Supervised learning","tag":"Selection by rank"}]},{"id":"1905.07189v2","table_header":["System","All P","All R","All F1","In  E+ P","In  E+ R","In  E+ F1"],"table":[["Name matching","15.03","15.03","15.03","29.13","29.13","29.13"],["MIL (model 1)","35.87","35.87","35.87 ±0.72","69.38","69.38","69.38 ±1.29"],["MIL-ND (model 2)","37.42","37.42","37.42 ±0.35","72.50","72.50","72.50 ±0.68"],["τMIL-ND (model 2)","38.91","36.73","37.78 ±0.26","73.19","71.15","72.16 ±0.48"],["Supervised learning","42.90","42.90","42.90 ±0.59","83.12","83.12","83.12 ±1.15"]],"caption":"Table 2: Results on the test set under two settings. 95% confidence intervals of F1 scores are shown.","description":"Table 2 shows results on the test set. 'Name matching' is far behind the two models.  MIL-ND achieves higher precision, recall, and F1 than MIL,  Using its confidence at test time (τ MIL-ND, 'All' setting) was also beneficial in terms of precision and F1 (it cannot possibly increase recall). Because all the test data points are valid for the 'In E+' setting, using the ND classifier had a slight negative effect on F1.  MIL-ND significantly outperforms MIL: the 95% confidence intervals for them do not overlap. However, this is not the case for MIL-ND and τ MIL-ND.","qa":[{"ques":"What is the value of R under the In E+ setting for the MIL(model 1) system?","ans":"69.38","tag":"Cell Selection(I)"},{"ques":"Under All setting highest value for R was observed for which system?","ans":"Supervised learning","tag":"Selection by rank"},{"ques":"What is the value of F1 under the In E+ setting for the MIL-ND(model 2) system?","ans":"72.50 ±0.68","tag":"Cell Selection(I)"},{"ques":"Under In E+ setting lowest value for P was observed for which system?","ans":"Name matching","tag":"Selection by rank"}]},{"id":"1909.00352v1","table_header":["Model","REF ⇒ GEN ENT","REF ⇒ GEN CON","REF ⇒ GEN NEU"],"table":[["S2S","38.45","11.17","50.38"],["G2S-GIN","49.78","9.80","40.42"],["G2S-GAT","49.48","8.09","42.43"],["G2S-GGNN","51.32","8.82","39.86"],["[EMPTY]","GEN ⇒ REF","GEN ⇒ REF","GEN ⇒ REF"],["Model","ENT","CON","NEU"],["S2S","73.79","12.75","13.46"],["G2S-GIN","76.27","10.65","13.08"],["G2S-GAT","77.54","8.54","13.92"],["G2S-GGNN","77.64","9.64","12.72"]],"caption":"Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.","description":"We perform an entailment experiment using BERT (Devlin et al., 2019) fine-tuned on the MultiNLI dataset (Williams et al., 2018) as a NLI model. We are interested in exploring whether a generated sentence (hypothesis) is semantically entailed by the reference sentence (premise). In a related text generation task, Falke et al. (2019) employ NLI models to rerank alternative predicted abstractive summaries.  Table 6 shows the average probabilities for entailment, contradiction and neutral classes on the LDC2017T10 test set. All G2S models have  higher entailment compared to S2S. G2S-GGNN has 33.5% and 5.2% better entailment performances than S2S, when REF entails GEN and GEN entails REF, respectively. G2S models also generate sentences that contradict the reference sentences less. This suggests that our models are capable of capturing better semantic information from the graph generating outputs semantically related to the reference sentences.","qa":[{"ques":"When the premise is generated, what is the CON value for the S2S model?","ans":"11.17","tag":"Cell Selection(II)"},{"ques":"For GEN->REF, what is the ENT value for the G2S-GIN model?","ans":"76.27","tag":"Cell Selection(I)"},{"ques":"When the hypothesis is generated, what is the NEU value for the G2S-GAT model?","ans":"13.92","tag":"Cell Selection(II)"},{"ques":"What is the lowest contradiction average percentage, when premise is generated?","ans":"8.09","tag":"Cell Selection(II) and Selection by rank"}]},{"id":"1909.00352v1","table_header":["Model","BLEU","METEOR"],"table":[["LDC2015E86","LDC2015E86","LDC2015E86"],["Konstas et al. (2017)","22.00","-"],["Song et al. (2018)","23.28","30.10"],["Cao et al. (2019)","23.50","-"],["Damonte et al.(2019)","24.40","23.60"],["Guo et al. (2019)","25.70","-"],["S2S","22.55 ± 0.17","29.90 ± 0.31"],["G2S-GIN","22.93 ± 0.20","29.72 ± 0.09"],["G2S-GAT","23.42 ± 0.16","29.87 ± 0.14"],["G2S-GGNN","24.32 ± 0.16","30.53 ± 0.30"],["LDC2017T10","LDC2017T10","LDC2017T10"],["Back et al. (2018)","23.30","-"],["Song et al. (2018)","24.86","31.56"],["Damonte et al.(2019)","24.54","24.07"],["Cao et al. (2019)","26.80","-"],["Guo et al. (2019)","27.60","-"],["S2S","22.73 ± 0.18","30.15 ± 0.14"],["G2S-GIN","26.90 ± 0.19","32.62 ± 0.04"],["G2S-GAT","26.72 ± 0.20","32.52 ± 0.02"],["G2S-GGNN","27.87 ± 0.15","33.21 ± 0.15"]],"caption":"Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.","description":"Table 2 shows the comparison between the proposed models, the baseline and other neural models on the test set of the two datasets.  For both datasets, our approach substantially outperforms the baselines. In LDC2015E86, G2S-GGNN achieves a BLEU score of 24.32, 4.46% higher than Song et al. (2018), who also use the copy mechanism. This indicates that our architecture can learn to generate better signals for text generation. On the same dataset, we have competitive results to Damonte and Cohen (2019). However, we do not rely on preprocessing anonymisation not to lose semantic signals. In LDC2017T10, G2S-GGNN achieves a BLEU score of 27.87, which is 3.33 points higher than Damonte and Cohen (2019), a state-of-the-art model that does not employ external information. We also have competitive results to Guo et al. (2019), a very recent state-of-the-art model.  We also outperform Cao and Clark (2019) improving BLEU scores by 3.48% and 4.00%, in LDC2015E86 and LDC2017T10, respectively. In contrast to their work, we do not rely on (i) leveraging supplementary syntactic information and (ii) we do not require an anonymization preprocessing step. G2S-GIN and G2S-GAT have comparable performance on both datasets. Interestingly, G2S-GGNN has better performance among our models. This suggests that graph encoders based on gating mechanisms are very effective in text generation models. We hypothesize that the gating mechanism can better capture longdistance dependencies between nodes far apart in the graph.","qa":[{"ques":"What is the BLEU score for the test set of LDC2015E86 on Cao et al. model?","ans":"23.5","tag":"Cell Selection(I)"},{"ques":"What is the METEOR score for the test set of LDC2015E86 on the Damonte et al. model?","ans":"23.6","tag":"Cell Selection(I)"},{"ques":"Which among our models performs the best on the LDC2015E86 test dataset?","ans":"G2S-GGNN model","tag":"Cell Selection(II)"},{"ques":"What are the BLEU and METEOR scores achieved by the G2S-GGNN model for the LDC2017T10 test dataset?","ans":"BLEU score- 27.87± 0.15, METEOR score- 33.21 ± 0.15","tag":"Cell Selection(I)"}]},{"id":"1909.00352v1","table_header":["Model","External","BLEU"],"table":[["Konstas et al. (2017)","200K","27.40"],["Song et al. (2018)","200K","28.20"],["Guo et al. (2019)","200K","31.60"],["G2S-GGNN","200K","32.23"]],"caption":"Table 3: Results on LDC2015E86 test set when models are trained with additional Gigaword data.","description":"We follow the method of Konstas et al. (2017), which is fine-tuning the model on the LDC2015E86 training set after every epoch of pretraining on the Gigaword data. G2S-GGNN outperforms others with the same amount of Gigaword sentences (200K), achieving a 32.23 BLEU score, as shown in Table 3. The results demonstrate that pretraining on automatically generated AMR graphs enhances the performance of our model.","qa":[{"ques":"What is the BLEU score for Konstas et al. model?","ans":"27.4","tag":"Cell Selection(I)"},{"ques":"Models are trained on how many additional Gigaword data?","ans":"200K","tag":"Cell Selection(II)"},{"ques":"Which baseline model achieves the highest BLEU score?","ans":"Guo et al. (2019)","tag":"Cell Selection(II)"},{"ques":"What is the BLEU score achieved by our model?","ans":"32.23","tag":"Cell Selection(II)"}]},{"id":"1909.00352v1","table_header":["Model","BLEU","METEOR","Size"],"table":[["biLSTM","22.50","30.42","57.6M"],["GEt + biLSTM","26.33","32.62","59.6M"],["GEb + biLSTM","26.12","32.49","59.6M"],["GEt + GEb + biLSTM","27.37","33.30","61.7M"]],"caption":"Table 4: Results of the ablation study on the LDC2017T10 development set.","description":"In Table 4, we report the results of an ablation study on the impact of each component of our model on the development set of LDC2017T10 dataset by removing the graph encoders. We also report the number of parameters (including embeddings) used in each model. The first thing we notice is the huge increase in metric scores (17% in BLEU) when applying the graph encoder layer, as the neural model receives signals regarding the graph structure of the input. The dual representation helps the model with a different view of the graph, increasing BLEU and METEOR scores by 1.04 and 0.68 points, respectively. The complete model has slightly more parameters than the model without graph encoders (57.6M vs 61.7M).","qa":[{"ques":"What is the BLEU score for biLSTM model?","ans":"22.50","tag":"Cell Selection(I)"},{"ques":"What are the number of parameters used in the GEt + biLSTM model?","ans":"59.6M","tag":"Cell Selection(II)"},{"ques":"Which model gives the highest BLEU score?","ans":"GEt + GEb + biLSTM","tag":"Selection by rank"},{"ques":"What is the least METEOR score observed?","ans":"30.42","tag":"Selection by rank"}]},{"id":"1909.00352v1","table_header":["Model","Graph Diameter 0-7 Δ","Graph Diameter 7-13 Δ","Graph Diameter 14-20 Δ"],"table":[["S2S","33.2","29.7","28.8"],["G2S-GIN","35.2 +6.0%","31.8 +7.4%","31.5 +9.2%"],["G2S-GAT","35.1 +5.9%","32.0 +7.8%","31.5 +9.51%"],["G2S-GGNN","36.2 +9.0%","33.0 +11.4%","30.7 +6.7%"],["[EMPTY]","Sentence Length","Sentence Length","Sentence Length"],["[EMPTY]","0-20 Δ","20-50 Δ","50-240 Δ"],["S2S","34.9","29.9","25.1"],["G2S-GIN","36.7 +5.2%","32.2 +7.8%","26.5 +5.8%"],["G2S-GAT","36.9 +5.7%","32.3 +7.9%","26.6 +6.1%"],["G2S-GGNN","37.9 +8.5%","33.3 +11.2%","26.9 +6.8%"],["[EMPTY]","Max Node Out-degree","Max Node Out-degree","Max Node Out-degree"],["[EMPTY]","0-3 Δ","4-8 Δ","9-18 Δ"],["S2S","31.7","30.0","23.9"],["G2S-GIN","33.9 +6.9%","32.1 +6.9%","25.4 +6.2%"],["G2S-GAT","34.3 +8.0%","32.0 +6.7%","22.5 -6.0%"],["G2S-GGNN","35.0 +10.3%","33.1 +10.4%","22.2 -7.3%"]],"caption":"Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.","description":"Table 5 shows METEOR5 scores for the LDC2017T10 dataset.  The performances of all models decrease as the diameters of the graphs increase. G2S-GGNN has a 17.9% higher METEOR score in graphs with a diameter of at most 7 compared to graphs with diameters higher than 13. This is expected as encoding a bigger graph (containing more information) is harder than encoding smaller graphs.  Since the models have fewer examples of bigger graphs to learn from, this also leads to worse performance when handling graphs with higher diameters. We also investigate the performance with respect to the sentence length. The models have better results when handling sentences with 20 or fewer tokens. Longer sentences pose additional challenges to the models.  G2S-GIN has a better performance in handling graphs with node out-degrees higher than 9. This indicates that GINs can be employed in tasks where the distribution of node degrees has a long tail. Surprisingly, S2S has a better performance than G2S-GGNN and G2S-GAT when handling graphs that contain high degree nodes.","qa":[{"ques":"Which model has the best performance for graph diameters in range 7-13?","ans":"G2S-GGNN model","tag":"Selection by rank"},{"ques":"As the sentence length is increasing, the performance decreases or increases?","ans":"Decreases","tag":"Cell Selection(II)"},{"ques":"What is the highest METEOR score observed for Max Node Out-degree of 4-8?","ans":"33.1 +10.4%","tag":"Selection by rank"},{"ques":"What is the highest METEOR score observed?","ans":"37.9 +8.5%","tag":"Selection by rank"}]},{"id":"1909.00352v1","table_header":["Model","ADDED","MISS"],"table":[["S2S","47.34","37.14"],["G2S-GIN","48.67","33.64"],["G2S-GAT","48.24","33.73"],["G2S-GGNN","48.66","34.06"],["GOLD","50.77","28.35"],["[EMPTY]","[EMPTY]","[EMPTY]"]],"caption":"Table 8: Fraction of elements in the output that are not present in the input (ADDED) and the fraction of elements in the input graph that are missing in the generated sentence (MISS), for the test set of LDC2017T10. The token lemmas are used in the comparison. GOLD refers to the reference sentences.","description":"We also evaluate the semantic adequacy of our model (how well does the generated output match the input?) by comparing the number of added and missing tokens that occur in the generated versus reference sentences (GOLD). As shown in Table 8, G2S approaches outperform the S2S baseline. G2S-GIN is closest to GOLD with respect to both metrics suggesting that this model is better able to generate novel words to construct the sentence and captures a larger range of concepts from the input AMR graph, covering","qa":[{"ques":"What is the MISS value for the S2S model?","ans":"37.14","tag":"Cell Selection(I)"},{"ques":"What is the least MISS value observed?","ans":"GOLD","tag":"Selection by rank"},{"ques":"What are the ADDED and MISS values observed for the GOLD model?","ans":"50.77 and 28.35","tag":"Cell Selection(I)"},{"ques":"What is the highest ADDED value observed?","ans":"50.77","tag":"Selection by rank"}]},{"id":"1801.07772v1","table_header":["[EMPTY]","Ar","Es","Fr","Ru","Zh","En"],"table":[["POS","88.7","90.0","89.6","88.6","87.4","85.2"],["SEM","85.3","86.1","85.8","85.2","85.0","80.7"]],"caption":"Table 4: SEM and POS tagging accuracy using features extracted from the 4th NMT encoding layer, trained with different target languages on a smaller parallel corpus (200K sentences).","description":"we trained systems using a smaller data size (200K sentences),  The results are shown in Table 4.  we observe a variance in classifier accuracy of 1-2%, based on target language,  This is true for both POS and SEM tagging.","qa":[{"ques":"What is the POS tagging accuracy for Ar language?","ans":"88.7","tag":"Cell Selection(I)"},{"ques":"What is the SEM tagging accuracy for Es language?","ans":"86.1","tag":"Cell Selection(I)"},{"ques":"For which language has the highest POS tagging accuracy observed?","ans":"Es","tag":"Selection by rank"},{"ques":"What are the POS and SEM tagging accuracy of Ru?","ans":"88.6 and 85.8","tag":"Cell Selection(I)"}]},{"id":"1801.07772v1","table_header":["[EMPTY]","MFT","UnsupEmb","Word2Tag"],"table":[["POS","91.95","87.06","95.55"],["SEM","82.00","81.11","91.41"]],"caption":"Table 2: POS and SEM tagging accuracy with baselines and an upper bound. MFT: most frequent tag; UnsupEmb: classifier using unsupervised word embeddings; Word2Tag: upper bound encoder-decoder.","description":"we consider two baselines: most frequent tag (MFT) for each word according to the training set (with the global majority tag for unseen words); and unsupervised word embeddings (UnsupEmb) as features for the classifier,  We also report an upper bound of directly training an encoderdecoder on word-tag sequences (Word2Tag),  Table 2 shows baseline and upper bound results. The UnsupEmb baseline performs rather poorly on both POS and SEM tagging.  NMT word embeddings (Table 3, rows with k = 0) perform slightly better,  the results are still below the most frequent tag baseline (MFT),  This is above the UnsupEmb baseline but only on par with the MFT baseline (Table 2).  The results are also far below the Word2Tag upper bound (Table 2).  which is far above the UnsupEmb and MFT baselines. While these results are below the oracle Word2Tag results (Table 2),","qa":[{"ques":"What is the POS tagging accuracy for MFT?","ans":"91.95","tag":"Cell Selection(I)"},{"ques":"What is the SEM tagging accuracy for a classifier using unsupervised word embeddings?","ans":"81.11","tag":"Cell Selection(II)"},{"ques":"What is the POS tagging accuracy for a classifier using upper bound encoder-decoder?","ans":"95.55","tag":"Cell Selection(II)"},{"ques":"What is the least POS tagging accuracy observed?","ans":"87.06","tag":"Selection by rank"}]},{"id":"1801.07772v1","table_header":["k","Ar","Es","Fr","Ru","Zh","En"],"table":[["POS Tagging Accuracy","POS Tagging Accuracy","POS Tagging Accuracy","POS Tagging Accuracy","POS Tagging Accuracy","POS Tagging Accuracy","POS Tagging Accuracy"],["0","88.0","87.9","87.9","87.8","87.7","87.4"],["1","92.4","91.9","92.1","92.1","91.5","89.4"],["2","91.9","91.8","91.8","91.8","91.3","88.3"],["3","92.0","92.3","92.1","91.6","91.2","87.9"],["4","92.1","92.4","92.5","92.0","90.5","86.9"],["SEM Tagging Accuracy","SEM Tagging Accuracy","SEM Tagging Accuracy","SEM Tagging Accuracy","SEM Tagging Accuracy","SEM Tagging Accuracy","SEM Tagging Accuracy"],["0","81.9","81.9","81.8","81.8","81.8","81.2"],["1","87.9","87.7","87.8","87.9","87.7","84.5"],["2","87.4","87.5","87.4","87.3","87.2","83.2"],["3","87.8","87.9","87.9","87.3","87.3","82.9"],["4","88.3","88.6","88.4","88.1","87.7","82.1"],["BLEU","BLEU","BLEU","BLEU","BLEU","BLEU","BLEU"],["[EMPTY]","32.7","49.1","38.5","34.2","32.1","96.6"]],"caption":"Table 3: SEM and POS tagging accuracy using features extracted from the k-th encoding layer of 4-layered NMT models trained with different target languages. “En” column is an English autoencoder. BLEU scores are given for reference.","description":"NMT word embeddings (Table 3, rows with k = 0) perform slightly better,  Table 3 summarizes the results of training classifiers to predict POS and SEM tags using features extracted from different encoding layers of 4  layered NMT systems.3 In the POS tagging results (first block), as the representations move above layer 0, performance jumps to around 91–92%.  This is above the UnsupEmb baseline but only on par with the MFT baseline (Table 2).  The results are also far below the Word2Tag upper bound (Table 2).  Comparing layers 1 through 4, we see that in 3\/5 target languages (Ar, Ru, Zh), POS tagging accuracy peaks at layer 1 and does not improve at higher layers, with some drops at layers 2 and 3. In 2\/5 cases (Es, Fr) the performance is higher at layer 4.  Turning to SEM tagging (Table 3, second block), representations from layers 1 through 4 boost the performance to around 87-88%,  which is far above the UnsupEmb and MFT baselines. While these results are below the oracle Word2Tag results (Table 2),  Going beyond the 1st encoding layer, representations from the 2nd and 3rd layers do not consistently improve semantic tagging performance. However, representations from the 4th layer lead to significant improvement with all target languages except for Chinese.  we found that En-En encoder-decoders (that is, English autoencoders) produce poor representations for POS and SEM tagging (last column in Table 3). This is especially true with higher layer representations (e.g. around 5% below the MT models using representations from layer 4). In contrast, the autoencoder has excellent sentence recreation capabilities (96.6 BLEU).  Table 3 also shows results using features obtained by training NMT systems on different target languages (the English source remains fixed). In both POS and SEM tagging, there are very small differences with different target languages (∼0.5%), except for Chinese which leads to slightly worse representations. While the differences are small,","qa":[{"ques":"What is the POS tagging accuracy for the 2nd encoding layer for Ar language?","ans":"","tag":"Cell Selection(II)"},{"ques":"For which encoding layer highest POS tagging accuracy for Ar language is achieved?","ans":"1st layer","tag":"Selection by rank"},{"ques":"What is the highest SEM tagging accuracy for the En language?","ans":"84.5","tag":"Cell Selection(II)"},{"ques":"What is the SEM tagging accuracy for the 3rd encoding layer for Zh language?","ans":"87.3","tag":"Selection by rank"}]},{"id":"1801.07772v1","table_header":["Uni","POS","0 87.9","1 92.0","2 91.7","3 91.8","4 91.9"],"table":[["Uni","SEM","81.8","87.8","87.4","87.6","88.2"],["Bi","POS","87.9","93.3","92.9","93.2","92.8"],["Bi","SEM","81.9","91.3","90.8","91.9","91.9"],["Res","POS","87.9","92.5","91.9","92.0","92.4"],["Res","SEM","81.9","88.2","87.5","87.6","88.5"]],"caption":"Table 5: POS and SEM tagging accuracy with features from different layers of 4-layer Uni\/Bidirectional\/Residual NMT encoders, averaged over all non-English target languages.","description":"improvements in both translation (+1-2 BLEU) and SEM tagging quality (+3-4% accuracy), across the board, when using a bidirectional encoder. Some of our bidirectional models obtain 92-93% accuracy. We observed similar improvements on POS tagging. Comparing POS and SEM tagging (Table 5), we note that higher layer representations improve SEM tagging, while POS tagging peaks at layer 1. we noticed small but consistent improvements in both translation (+0.9 BLEU) and POS and SEM tagging (up to +0.6% accuracy) when using features extracted from an NMT model trained with residual connections (Table 5). We also observe similar trends as before: POS tagging does not benefit from features from the upper layers, while SEM tagging improves with layer 4 representations.","qa":[{"ques":"What is the POS tagging accuracy for 2nd layer Unidirectional NMT encoder?","ans":"91.7","tag":"Cell Selection(II)"},{"ques":"What is the highest POS tagging accuracy in Bidirectional NMT encoder?","ans":"93.3","tag":"Selection by rank"},{"ques":"What is the highest SEM tagging accuracy in Residual NMT encoder?","ans":"88.5","tag":"Selection by rank"},{"ques":"What is the SEM value under column name 3 for row name Bi?","ans":"91.9","tag":"Cell Selection(I)"}]},{"id":"1808.06640v2","table_header":["Data","Task","Protected Attribute","Δ"],"table":[["Dial","Sentiment","Race","12.2"],["[EMPTY]","Mention","Race","14.3"],["PAN16","Mention","Gender","8.1"],["[EMPTY]","Mention","Age","9.7"]],"caption":"Table 8: Attacker’s performance on different datasets. Results are on a training set 10% held-out. Δ is the difference between the attacker score and the corresponding adversary’s accuracy.","description":"Table 8 summarize the training accuracies of the attacker network. The Mention\/Race task achieves the highest score of 64.3% whereas the Mention\/Gender task achieves the lowest 58.1%.","qa":[{"ques":"What is the difference between the attacker score and the corresponding adversary’s accuracy for DIAL data sentiment Task?","ans":"12.2","tag":"Cell Selection(II)"},{"ques":"What is the protected attribute for ∆ of 8.1?","ans":"Gender","tag":"Cell Selection(I)"},{"ques":"For PAN16 data, mention task, what is the highest ∆?","ans":"9.7","tag":"Selection by rank"},{"ques":"Mention how many unique protected attributes are there in DIAL data?","ans":"1","tag":"Aggregate selection"}]},{"id":"1808.06640v2","table_header":["Data","Task","Accuracy"],"table":[["Dial","Sentiment","67.4"],["[EMPTY]","Mention","81.2"],["[EMPTY]","Race","83.9"],["PAN16","Mention","77.5"],["[EMPTY]","Gender","67.7"],["[EMPTY]","Age","64.8"]],"caption":"Table 1: Accuracies when training directly towards a single task.","description":"The results in Table 1 indicate that the classifiers achieve reasonable accuracies for the main tasks.  the protected attributes, race is highly predictable (83.9%) while age and gender can also be recovered at above 64% accuracy.","qa":[{"ques":"For DIAL data, sentiment task what is the accuracy achieved?","ans":"67.4","tag":"Cell Selection(I)"},{"ques":"For DIAL data what is the highest accuracy achieved?","ans":"83.9","tag":"Selection by rank"},{"ques":"For the PAN16 data, age task what is the accuracy achieved?","ans":"64.8","tag":"Cell Selection(I)"},{"ques":"For PAN16 data, for which task is the highest accuracy achieved?","ans":"Mention task","tag":"Selection by rank"}]},{"id":"1808.06640v2","table_header":["Data","Task","Protected Attribute","Balanced Task Acc","Balanced Leakage","Unbalanced Task Acc","Unbalanced Leakage"],"table":[["Dial","Sentiment","Race","67.4","64.5","79.5","73.5"],["[EMPTY]","Mention","Race","81.2","71.5","86.0","73.8"],["PAN16","Mention","Gender","77.5","60.1","76.8","64.0"],["[EMPTY]","[EMPTY]","Age","74.7","59.4","77.5","59.7"]],"caption":"Table 2: Protected attribute leakage: balanced & unbalanced data splits.","description":"This experiment suggests an upper bound on the amount of leakage of protected attributes when we do not actively attempt to prevent it. The Balanced section in Table 2 summarizes the validation-set accuracies. While the numbers are lower than when training directly (Table 1), they are still high enough to extract meaningful and possibly highly sensitive information (e.g. DIAL Race direct prediction is 83.9% while DIAL Race leakage on the balanced Sentiment task is 64.5%).  We simulate this more realistic scenario by constructing unbalanced datasets in which the main tasks (sentiment\/mention) remain balanced but the protected class proportions within each main class are not, as demonstrated in Figure 1b.  We then follow the leakage experiment on the unbalanced datasets. The attacker is trained and tested on a balanced dataset. Otherwise, the attacker can perform quite well on the male\/female task simply by learning to predict sentiment, which does not reflect leakage of gender data to the representation. When training the attacker on balanced data, its decisions cannot rely on the sentiment information encoded in the vectors, and must look for encoded information about the protected attributes. The results in Table 2 indicate that both task accuracy and attribute leakage are stronger in the unbalanced case.","qa":[{"ques":"For DIAL data what is the balanced task acc?","ans":"67.4","tag":"Cell Selection(I)"},{"ques":"For PAN16 data what is the unbalanced leakage for the protected attribute age?","ans":"59.7","tag":"Cell Selection(I)"},{"ques":"Mention all the unique protected attributes?","ans":"Race,gender,age","tag":"Aggregate selection"},{"ques":"What is the highest unbalanced task acc value?","ans":"86.0","tag":"Selection by rank"}]},{"id":"1808.06640v2","table_header":["Data","Task","Protected Attribute","Task Acc","Leakage","Δ"],"table":[["Dial","Sentiment","Race","64.7","56.0","5.0"],["[EMPTY]","Mention","Race","81.5","63.1","9.2"],["PAN16","Mention","Gender","75.6","58.5","8.0"],["[EMPTY]","Mention","Age","72.5","57.3","6.9"]],"caption":"Table 3: Performances on different datasets with an adversarial training. Δ is the difference between the attacker score and the corresponding adversary’s accuracy.","description":"However, training the attacker network on the resulting encoder vectors reveals a different story. For example, when considering the encoder after 50 training epochs (adversary accuracy of 49.0%), the attacker reaches 56.0% accuracy: substantially higher than the adversarial's success rate, despite sharing the exact same architecture, and being trained and tested on the exact same dataset. Table 3 summarizes the attacker's recovery rate on the adversarialy-trained encoders for the different settings. In all cases, the adversarial's success rate is around 50%, while the attacker's rate is substantially higher.","qa":[{"ques":"What is the highest difference between the attacker score and the corresponding adversary’s accuracy?","ans":"9.2","tag":"Selection by rank and Cell Selection(II)"},{"ques":"What is the least task acc value observed?","ans":"64.7","tag":"Selection by rank"},{"ques":"What are all unique tasks?","ans":"sentiment, mention","tag":"Aggregate selection"},{"ques":"What is the leakage value for mention task under DIAL data?","ans":"63.1","tag":"Cell Selection(I)"}]},{"id":"1808.06640v2","table_header":["[EMPTY]","[EMPTY]","Embedding Leaky","Embedding Guarded"],"table":[["RNN","Leaky","64.5","67.8"],["RNN","Guarded","59.3","54.8"]],"caption":"Table 6: Accuracies of the protected attribute with different encoders.","description":"We compare encoders Leaky-EMB and Leaky-RNN to gauge which module has a greater contribution to the data leakage.  Table 6 summarize the results, implying that the leakage is caused mainly by the RNN, and less by the Embedding Matrix.","qa":[{"ques":"What is the value of RNN row and guarded embedding column?","ans":"67.8","tag":"Cell Selection(I)"},{"ques":"Under leaky column what is the highest value?","ans":"64.5","tag":"Selection by rank"},{"ques":"What is the sum of all values in the table?","ans":"246.4","tag":"Basic arithmetic operators"}]},{"id":"1905.13324v1","table_header":["Model","Model","#Params","PTB Base","PTB +Finetune","PTB +Dynamic","WT2 Base","WT2 +Finetune","WT2 +Dynamic"],"table":[["Yang et al. ( 2018 )","Yang et al. ( 2018 )","22M","55.97","54.44","47.69","63.33","61.45","40.68"],["This","LSTM","22M","63.78","62.12","53.11","69.78","68.68","44.60"],["This","GRU","17M","69.09","67.61","60.21","73.37","73.05","49.77"],["This","ATR","9M","66.24","65.86","58.29","75.36","73.35","48.65"],["Work","SRU","13M","69.64","65.29","60.97","85.15","84.97","57.97"],["[EMPTY]","LRN","11M","61.26","61.00","54.45","69.91","68.86","46.97"]],"caption":"Table 5: Test perplexity on PTB and WT2 language modeling task. “#Params”: the parameter number in PTB task. Finetune: fintuning the model after convergence. Dynamic dynamic evaluation. Lower perplexity indicates better performance.","description":"Table 5 shows the test perplexity of different models.10 In this task, LRN significantly outperforms GRU, ATR and SRU, and achieves near the same perplexity as LSTM.","qa":[{"ques":"What is the test perplexity on the PTB language modeling task under finetune column for ATR model?","ans":"65.86","tag":"Cell Selection(II)"},{"ques":"What is the test perplexity on the WT2 language modeling task under dynamic column for SRU model?","ans":"57.97","tag":"Cell Selection(II)"},{"ques":"Which model has the best performance on the WT2 language modeling task under dynamic column?","ans":"LSTM","tag":"Cell Selection(II)"},{"ques":"Which model has the best performance on the PTB language modeling task under finetune column?","ans":"LRN","tag":"Cell Selection(II)"}]},{"id":"1905.13324v1","table_header":["Model","Model","#Params","Base ACC","Base Time","+LN ACC","+LN Time","+BERT ACC","+BERT Time","+LN+BERT ACC","+LN+BERT Time"],"table":[["Rocktäschel et al. ( 2016 )","Rocktäschel et al. ( 2016 )","250K","83.50","-","-","-","-","-","-","-"],["This","LSTM","8.36M","84.27","0.262","86.03","0.432","89.95","0.544","90.49","0.696"],["This","GRU","6.41M","85.71","0.245","86.05","0.419","90.29","0.529","90.10","0.695"],["This","ATR","2.87M","84.88","0.210","85.81","0.307","90.00","0.494","90.28","0.580"],["Work","SRU","5.48M","84.28","0.258","85.32","0.283","89.98","0.543","90.09","0.555"],["[EMPTY]","LRN","4.25M","84.88","0.209","85.06","0.223","89.98","0.488","89.93","0.506"]],"caption":"Table 1: Test accuracy (ACC) on SNLI task. “#Params”: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.","description":"Table 1 shows the test accuracy and training time of different models. Our implementation outperforms the original model where Rockt¨aschel et al. (2016) report an accuracy of 83.50. Overall results show that LRN achieves competitive performance but consumes the least training time. Although LSTM and GRU outperform LRN by 0.3∼0.9 in terms of accuracy, these recurrent units sacrifice running efficiency (about 7%∼48%) depending on whether LN and BERT are applied. No significant performance difference is observed between SRU and LRN, but LRN has fewer model parameters and shows a speedup over SRU of 8%∼21%.  However, for LSTM, GRU and ATR, LN results in significant computational overhead (about 27%∼71%). In contrast, quasi recurrent models like SRU and LRN only suffer a marginal speed decrease.  Results with BERT show that contextual information is valuable for performance improvement. LRN obtains additional 4 percentage points gain with BERT and reaches an accuracy of around 89.9.  In addition, although the introduction of BERT brings in heavy matrix computation, the benefits from LRN do not disappear. LRN is still the fastest model, outperforming other recurrent units by 8%∼27%.","qa":[{"ques":"What is the test accuracy for the layer normalization model, under the time column for GRU?","ans":"0.419","tag":"Cell Selection(II)"},{"ques":"What is the test accuracy for the BERT model, under the ACC column for SRU?","ans":"89.98","tag":"Cell Selection(II)"},{"ques":"What is the highest test accuracy for the base model under the ACC column?","ans":"85.71","tag":"Cell Selection(II) and selection by rank"},{"ques":"For which model is the highest test accuracy observed under +LN+BERT under time column?","ans":"LSTM","tag":"Cell Selection(II) and selection by rank"}]},{"id":"1905.13324v1","table_header":["Model","Model","#Params","AmaPolar ERR","AmaPolar Time","Yahoo ERR","Yahoo Time","AmaFull ERR","AmaFull Time","YelpPolar ERR","YelpPolar Time"],"table":[["Zhang et al. ( 2015 )","Zhang et al. ( 2015 )","-","6.10","-","29.16","-","40.57","-","5.26","-"],["This","LSTM","227K","4.37","0.947","24.62","1.332","37.22","1.003","3.58","1.362"],["This","GRU","176K","4.39","0.948","24.68","1.242","37.20","0.982","3.47","1.230"],["This","ATR","74K","4.78","0.867","25.33","1.117","38.54","0.836","4.00","1.124"],["Work","SRU","194K","4.95","0.919","24.78","1.394","38.23","0.907","3.99","1.310"],["[EMPTY]","LRN","151K","4.98","0.731","25.07","1.038","38.42","0.788","3.98","1.022"]],"caption":"Table 2: Test error (ERR) on document classification task. “#Params”: the parameter number in AmaPolar task. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti.","description":"Table 2 summarizes the classification results. LRN achieves comparable classification performance against ATR and SRU, but slightly  underperforms LSTM and GRU (-0.45∼-1.22).  LRN accelerates the training over LSTM and SRU by about 20%,","qa":[{"ques":"What is the test error for the AmaPolar model, under the time column for GRU?","ans":"0.948","tag":"Cell Selection(II)"},{"ques":"What is the test error for the AmaFull model, under the ERR column for ATR?","ans":"38.54","tag":"Cell Selection(II)"},{"ques":"What is the least test error for the Yahoo model under the ERR column?","ans":"24.62","tag":"Cell Selection(II) and selection by rank"},{"ques":"Which model has the best performance for YelpPolar under ERR column?","ans":"GRU","tag":"Cell Selection(II) and selection by rank"}]},{"id":"1905.13324v1","table_header":["Model","#Params","BLEU","Train","Decode"],"table":[["GNMT","-","24.61","-","-"],["GRU","206M","26.28","2.67","45.35"],["ATR","122M","25.70","1.33","34.40"],["SRU","170M","25.91","1.34","42.84"],["LRN","143M","26.26","0.99","36.50"],["oLRN","164M","26.73","1.15","40.19"]],"caption":"Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.","description":"The results in Table 3 show that translation quality of LRN is slightly worse than that of GRU (-0.02 BLEU).  however, oLRN yields the best BLEU score of 26.73, outperforming GRU (+0.45 BLEU). In addition, the training time results in Table 3 confirm the computational advantage of LRN over all other recurrent units, where LRN speeds up over ATR and SRU by approximately 25%.  the recurrent unit with the least computation operations, i.e. ATR, becomes the fastest. Still, both LRN and oLRN translate sentences faster than SRU (+15%\/+6%).","qa":[{"ques":"Which model takes the least time to decode one sentence measured on the newstest2014 dataset?","ans":"ATR","tag":"Cell Selection(II) and selection by rank"},{"ques":"What is the BLEU score for the GRU model?","ans":"26.28","tag":"Cell Selection(I)"},{"ques":"What is the least time per training batch measured from 0.2k training steps on Tesla P100?","ans":"0.99","tag":"Cell Selection(II) and selection by rank"},{"ques":"What is the #Params value for the oLRN model?","ans":"164M","tag":"Cell Selection(I)"}]},{"id":"1905.13324v1","table_header":["Model","#Params","Base","+Elmo"],"table":[["rnet*","-","71.1\/79.5","-\/-"],["LSTM","2.67M","70.46\/78.98","75.17\/82.79"],["GRU","2.31M","70.41\/ 79.15","75.81\/83.12"],["ATR","1.59M","69.73\/78.70","75.06\/82.76"],["SRU","2.44M","69.27\/78.41","74.56\/82.50"],["LRN","2.14M","70.11\/78.83","76.14\/ 83.83"]],"caption":"Table 4: Exact match\/F1-score on SQuad dataset. “#Params”: the parameter number of Base. rnet*: results published by Wang et al. (2017).","description":"Table 4 lists the EM\/F1 score of different models. In this task, LRN outperforms ATR and SRU in terms of both EM and F1 score. After integrating Elmo for contextual modeling, the performance of LRN reaches the best (76.1  EM and 83.83 F1), beating both GRU and LSTM (+0.33EM, +0.71F1).","qa":[{"ques":"What is the Exact match\/F1-score in the results published by Wang et al. (2017) for the base model?","ans":"71.1\/79.5","tag":"Cell Selection(II)"},{"ques":"What is the #Params value for the LRN model?","ans":"2.14M","tag":"Cell Selection(I)"},{"ques":"After integrating Elmo, which model gives the highest Exact match\/F1-score?","ans":"LRN","tag":"Cell Selection(II) and selection by rank"},{"ques":"What is the Exact match\/F1-score for the ATR base model?","ans":"69.73\/78.70","tag":"Cell Selection(I)"}]},{"id":"1905.13324v1","table_header":["Model","#Params","NER"],"table":[["LSTM*","-","90.94"],["LSTM","245K","89.61"],["GRU","192K","89.35"],["ATR","87K","88.46"],["SRU","161K","88.89"],["LRN","129K","88.56"]],"caption":"Table 6: F1 score on CoNLL-2003 English NER task. “#Params”: the parameter number in NER task. LSTM* denotes the reported result Lample et al. (2016).","description":"As shown in Table 6, the performance of LRN matches that of ATR and SRU, though LSTM and GRU operate better (+1.05 and +0.79).","qa":[{"ques":"What is the F1 score achieved for the SRU model?","ans":"88.89","tag":"Cell Selection(I)"},{"ques":"What is the #Params value for the LRN model ?","ans":"129K","tag":"Cell Selection(I)"},{"ques":"Which model performed better, LSTM or GRU?","ans":"LSTM","tag":"Cell Selection(II) and selection by rank"},{"ques":"What is the F1 score reported by Lample et al., 2016?","ans":"90.94","tag":"Cell Selection(II)"}]}]